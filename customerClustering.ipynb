{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Title: Customer Segmentation\n",
    "#### Business understanding: \n",
    "The online sales data is available from retailers and is available at the [UCI website](https://archive.ics.uci.edu/dataset/352/online+retail) for analysis. The business expects to use the RFM methodology, data mining techniques, and machine learning algorithms to derive meaningful customer segmentation, better understand customer purchase behavior, and identify the characteristics of customers in each segment. \n",
    "\n",
    "#### Business Goal \n",
    "The Business goal is to organize customers in similar groups, better understand individual customers in each cluster, and identify the customers at risk. This way business can have a customer-centric focus & approach to target individual customers. \n",
    "\n",
    "#### Data Understanding: \n",
    "The sales data is downloaded from [UCI website](https://archive.ics.uci.edu/dataset/352/online+retail), it is a transactional data set that contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a retailer and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "The original data attributes are:\n",
    "\n",
    "|Feature Name | Description                                | Feature Type  |\n",
    "|-------------|--------------------------------------------|---------------|\n",
    "|InvoiceNo    | A unique number for a transaction          | Categorical   |\n",
    "|StockCode    | Product number for an Item                 | Categorical   |                          \n",
    "|Description  | Product Name                               | Categorical   |\n",
    "|Quantity     | Product quantity in each transaction       | Integer       |\n",
    "|InvoiceDate  | Day & Time when transaction was generated  | Date          |\n",
    "|UnitPrice    | Price of unit product                      | Continuous    |\n",
    "|CustomerID   | ID of customer                             | Categorical   |\n",
    "|Country      | Country where customer resides             | Categorical   |\n",
    "\n",
    "The data attributes added as part of Analysis & Feature Engineering are:\n",
    "\n",
    "|Feature Name \t  | Description                                                               | Feature Type  |\n",
    "|-----------------|---------------------------------------------------------------------------|---------------|\n",
    "| Recency      \t  | Define how recently the customer made a purchase                          | Integer       |    \n",
    "| Frequency    \t  | Define how often customers make purchases                                 | Integer       |\n",
    "| Monetary     \t  | Define the amount the customer has spent                                  | Float         |    \n",
    "| Recency Score\t  | Quantile-based discretization on a scale of 1-5 based on Recency value    | Integer       |\n",
    "| Frequency Score | Quantile-based discretization on a scale of 1-5 based on Frequency value  | Integer       |\n",
    "| Monetary Score  | Quantile-based discretization on a scale of 1-5 based on Frequency value  | Integer       |\n",
    "| RFM Segment     | ID of customer                                                            | Object        |            \n",
    "| Customer Type   | Country where customer resides                                            | Object        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "***\n",
    "\n",
    "1. [Import Libraries](#10-import-libraries)\n",
    "\n",
    "2. [Data Preparation & Feature Engineering](#20-data-preperation-and-feature-engineering) \n",
    "    - 2.1 -  [Read Data](#21-upload-the-dataset)\n",
    "    - 2.2 -  [Evaluate Feature Discrepancy & Missing Values](#22-evaluate-discrepancy-in-feature-values)\n",
    "    - 2.3 -  [Add New Features](#23-introduce-additional-features-using-rfm-technique)\n",
    "    - 2.4 -  [Trim Dataset](#24-trim-dataset-with-random-sampling)\n",
    "    - 2.5 -  [Encode & Scale Data](#25-encode-and-scale-the-dataset)\n",
    "    - 2.6 -  [Correlation Matrix - Feature Analysis](#26-correaltion-matrix--feature-analysis)\n",
    "    - 2.7 -  [Outlier Analysis](#27-outlier-analysis---box-plot)\n",
    "    - 2.8 -  [Outlier Treatment](#28-outlier-treatment)\n",
    "    - 2.9 -  [Model Dataset](#29-create-target-dataset-for-model)\n",
    "    - 2.10 - [Pair Plot - Analysis Of RMF Features](#210-pair-plot-to-analyse-rfm-features)\n",
    "\n",
    "3. [Unsupervised Learning Algorithm ](#30-execute-unsupervised-ml-alogrithm)\n",
    "    - 3.1 - [Kmeans Algorithm](#31-kmeans-algorithm)\n",
    "        - 3.1.1 - [Elbow Method - Identify Optimum Clusters](#311-elbow-method---identify-the-optimum-number-of-customer-clusters)\n",
    "        - 3.1.2 - [Elbow Method Analysis](#312-elbow-method-analysis)\n",
    "        - 3.1.3 - [Execute & Plot Kmeans](#313-execute-and-plot-kmeans-alogrithm)\n",
    "        - 3.1.4 - [Kmeans Cluster Analysis](#314-kmeans-cluster-analysis)\n",
    "        - 3.1.5 - [RFM Score per Cluster](#315-rfm-scores-per-cluster)\n",
    "    - 3.2 - [Silhouette Score](#32-silhouette-score--analysis)\n",
    "        - 3.2.1 - [Silhouette Plot](#321-generate-silhouette-score-and-plot-the-feature-dependency)\n",
    "        - 3.2.2 - [Silhouette Cluster Analysis](#322-analysis-of-silhouette-cofficient-and-clusters)\n",
    "    - 3.3 - [DBSCAN Algorithm](#33-dbscan-alogorithm)\n",
    "        - 3.3.1 - [Nearest Neighbor](#331-nearestneighbor---find-optimal-eps-parameter-for-dbscan)\n",
    "        - 3.3.2 - [DBSCAN Execution ](#332---execute-dbscan)\n",
    "        - 3.3.3 - [DBSCAN Result Plot](#333-plot-dbscan-result)\n",
    "        - 3.3.4 - [DBSCAN cluster Analysis](#334-analyze-dbscan-result)\n",
    "4. [Customer Cluster Analysis](#40-customer-cluster-analysis)\n",
    "    - 4.1 - [RFM Score per Cluster](#41-rfm-score-per-cluster)\n",
    "    - 4.2 - [Customer Count per Cluster](#42-customer-count-per-cluster)\n",
    "    - 4.3 - [Peak Purchase Time](#43-peak-purchase-time)\n",
    "    - 4.4 - [Top Selling Item](#44-cluster-wise-top-selling-months)\n",
    "    - 4.5 - [Top Selling Item per Cluster](#45-top-selling-items-per-cluster-and-overall)\n",
    "    - 4.6 - [Customer Segment TreeMap](#46-tree-map---customer-segement-based-on-rfm-score)\n",
    "        - 4.6.1 - [Dashboard - Customer Score Base ](#461-rfm-score-based-segment-tree-map)\n",
    "        - 4.6.2 - [Feature Creation- Customer Type based on RFM](#462-create-customer-type-column-based-on-rfm-score) \n",
    "        - 4.6.3 - [Dashboard - Customer Segment based on RFM](#463-tree-map-for-customer-segement)\n",
    "5. [Conclusion](#50-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "## Import the required library for the project\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import strftime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import OneHotEncoder, TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data preperation and Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Upload the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload the dataset \n",
    "df = pd.read_csv('OnlineRetail.csv')\n",
    "\n",
    "# Executing df info to get the information on dataframe\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Evaluate discrepancy in feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the rows having null values and evaluating the column values. \n",
    "#df.isnull().sum()\n",
    "\n",
    "## Evaluate any discrepancy in data with Quantity, UnitPrice, Country and Invoice date column \n",
    "\n",
    "for column in df.columns:\n",
    "    if column == 'Quantity':\n",
    "        print('Quantity with zero values:', df.query('Quantity == 0')[['Quantity']].count())\n",
    "    if column == 'InvoiceDate':\n",
    "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "        print('Min TimeStamp: ', df[['InvoiceDate']].min())\n",
    "        print('Max TimeStamp: ', df[['InvoiceDate']].max())\n",
    "    if column == 'UnitPrice':\n",
    "        print('Rows have zero unit price:',df.query('UnitPrice == 0')[['UnitPrice']].count())\n",
    "\n",
    "## As per analysis we can drop the Customer_ID and remove the rows with zero unit price. The decision on other columns will be taken at latter stage.\n",
    "\n",
    "## Drop the customer ID with Null values\n",
    "df.dropna(inplace=True) \n",
    "\n",
    "## Drop rows with quantity == zero\n",
    "df.drop(df.query('UnitPrice == 0')[['UnitPrice']].index, inplace=True, errors='ignore')\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Introduce additional features using RFM technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate additional features for the dataframe\n",
    "\n",
    "## There are invoices with returns, creating column Transaction type to identify such invoices.\n",
    "## The returns are maked with Transaction type as Return \n",
    "df['TransactionType'] = np.where(df['InvoiceNo'].str.startswith('C'), 'Return','Transaction')  \n",
    "\n",
    "## Introducing the Recency, Frequency & monitory column to evalute the purchase behaviour of the customer.\n",
    "# Recency — How recently did the customer purchase?\n",
    "# Frequency — How often do they purchase?\n",
    "# Monetary Value — How much do they spend?\n",
    "\n",
    "# Calculate the amount and create a column by multiplying quantity purchased by unit price  \n",
    "df['Amount'] = df['Quantity'] * df['UnitPrice'] \n",
    "\n",
    "## Seperate the date time to indentify the highest & least selling day, as well as peak selling hours\n",
    "\n",
    "# Create date and time column in format specified using strftime function\n",
    "df['Date'] = df['InvoiceDate'].dt.strftime(\"%Y%m%d\")\n",
    "df['Time'] = df['InvoiceDate'].dt.strftime(\"%H\")\n",
    "\n",
    "# Calculate customer Recency score\n",
    "max_date = df['InvoiceDate'].max()\n",
    "df['Recency'] = (max_date - df['InvoiceDate']).dt.days\n",
    "\n",
    "# Calculate custor shopping Frequency\n",
    "df_cust = df.groupby('CustomerID')['Date'].count().reset_index()\n",
    "df_cust.columns = ['CustomerID', 'Frequency']\n",
    "df = df.merge(df_cust, on='CustomerID')\n",
    "\n",
    "# Calculate customer Monetary\n",
    "df_monetary = df.groupby('CustomerID')['Amount'].sum().reset_index() \n",
    "df_monetary.columns = ['CustomerID', 'Monetary']\n",
    "df = df.merge(df_monetary, on='CustomerID',copy=True)\n",
    "\n",
    "# Calculate the scores\n",
    "for scores in ['Recency', 'Frequency','Monetary']:\n",
    "    if scores == 'Recency':\n",
    "        df[f\"{scores}_score\"] = pd.qcut(df[scores], 5, labels=list(range(5,0,-1)))\n",
    "    else:\n",
    "        df[f\"{scores}_score\"] = pd.qcut(df[scores], 5, labels=list(range(1,6,1)))\n",
    "    \n",
    "    df[f\"{scores}_score\"] = df[f\"{scores}_score\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 Trim dataset with random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run train test function to split the dataset\n",
    "\n",
    "_,X = train_test_split(df,test_size=0.25,random_state=42)\n",
    "X.reset_index(inplace=True)\n",
    "X_copy = X.copy()\n",
    "X.drop(['Description', 'InvoiceNo', 'StockCode', 'InvoiceDate','CustomerID','index',],axis=1,inplace=True,errors='ignore')\n",
    "\n",
    "# Define dummy y\n",
    "y = X['Recency']\n",
    "\n",
    "# Describe the dataset \n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5 Encode and Scale the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define pipeline to encode and scale the dataset \n",
    "\n",
    "# Set the column values\n",
    "col = X.columns\n",
    "\n",
    "# Column transformer to Encode the data \n",
    "pre_process = ColumnTransformer([\n",
    "                                (\"target_encoder\", TargetEncoder(), [\"TransactionType\", \"Country\"])]\n",
    "                                , remainder=\"passthrough\")\n",
    "\n",
    "## Pipeline JOB to execute the data encoder and scale the data. \n",
    "data = Pipeline([\n",
    "                   ('preprocess', pre_process), \n",
    "                   ('scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "# Run the pipeline with original dataset\n",
    "scaled_df = data.fit_transform(X,y)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=col)\n",
    "\n",
    "# Print pipeline step \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6 Correaltion matrix & feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate thecorelation matrix \n",
    "\n",
    "# Call corelation function\n",
    "correlation_matrix = scaled_df.corr()\n",
    "# Set the graph size\n",
    "plt.figure(figsize=(10,8))  # Adjust the figure size if needed\n",
    "# Execute the heatmap function\n",
    "seaborn.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "#Set the map title\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "# Draw the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7 Outlier Analysis - Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outlier Analysis of the new features  \n",
    "fig = go.Figure()\n",
    "\n",
    "# Draw the Box plot for outlier analysis \n",
    "fig.add_trace(go.Box(x=scaled_df['Recency'],   name='Recency Plot'))\n",
    "fig.add_trace(go.Box(x=scaled_df['Frequency'], name='Frequency Plot'))\n",
    "fig.add_trace(go.Box(x=scaled_df['Monetary'],  name='Monetary Plot'))\n",
    "# Update the title \n",
    "fig.update_layout(title_text=\"Box plot for Monetary, Frequency, Recency\")\n",
    "# Draw the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.8 Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outlier treatment of new features \n",
    "\n",
    "# calculate the first quartile\n",
    "Q1 = scaled_df.quantile(0.25)\n",
    "\n",
    "# calculate the third quartile\n",
    "Q3 = scaled_df.quantile(0.75)\n",
    "\n",
    "# The Interquartile Range (IQR) is defined as the difference between the third and first quartile\n",
    "# calculate IQR for each numeric variable\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# retrieve the dataframe without the outliers\n",
    "# '~' returns the values that do not satisfy the given conditions \n",
    "# i.e. it returns values between the range [Q1-1.5*IQR, Q3+1.5*IQR]\n",
    "# '|' is used as 'OR' operator on multiple conditions   \n",
    "# 'any(axis=1)' checks the entire row for atleast one 'True' entry (those rows represents outliers in the data)\n",
    "scaled_df = scaled_df[~((scaled_df < (Q1 - 1.5 * IQR)) | (scaled_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# check the shape of the data\n",
    "scaled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.9 Create Target Dataset for Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the target dataset for Model \n",
    "\n",
    "# Select the data based on index from scaled dataframe \n",
    "X = X.loc[scaled_df.index]\n",
    "# Make a copy of X, this will be used at latter stage for analysis\n",
    "X_copy = X_copy.loc[scaled_df.index]\n",
    "\n",
    "# Generate the Target Dataset with new feature RFM, this dataset will be primary used for Models   \n",
    "Target_Data = scaled_df[['Recency', 'Frequency','Monetary']]\n",
    "Target_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.10 Pair plot to analyse RFM features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pair plot to visualize the trend and dependency between RFM features \n",
    "\n",
    "# Run pairplot with scaled dataset \n",
    "pair = sns.pairplot(scaled_df[['Recency', 'Frequency','Monetary']], diag_kws={'color':'orange'}, plot_kws={'color':'blue'})\n",
    "pair.fig.suptitle(\"Pair plot for RFM features\", y=1.08)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Execute Unsupervised ML alogrithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Kmeans Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 Elbow Method - Identify the optimum number of customer clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Elbow method to Identify the optimum number of customer clusters \n",
    "\n",
    "# Initailize inertia \n",
    "inertia = []\n",
    "\n",
    "# Execute KMeans algorithm for 1-10 clusters, and capture the inertia from every execution \n",
    "for i in range(1, 11, 1):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42, n_init=\"auto\").fit(Target_Data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow graph\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.plot(np.ones(10)*5, inertia, linestyle='--')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 Elbow method analysis\n",
    "\n",
    "The plot of intertia & number of clusters shows bend at cluster 2 & 5. Picking 5 clusters to clearly distinguish the customer group. Also, we will evaluate the customer clusters using other techniques as well. After removing the outliers & scaling the data, the data seems to be more homogeneous.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 Execute and plot KMeans alogrithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute Kmeans Algorithm\n",
    "\n",
    "# Initialize the algorith and set the number of cluster to 5 based on above selection.\n",
    "model = KMeans(init='k-means++', n_clusters=5, random_state=42, n_init=\"auto\")\n",
    "\n",
    "# Execute the alogorithm on Target Dataset\n",
    "kmean = model.fit(Target_Data)\n",
    "cluster_lib = kmean.fit_predict(Target_Data)\n",
    "\n",
    "# Add cluster segment to original X & X_copy for analysing clusters at latter stage\n",
    "X['Cluster'] = cluster_lib\n",
    "X_copy['Cluster'] = cluster_lib\n",
    "\n",
    "# Plot 3D scatter graph for RFM features  \n",
    "# Set size\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "# Define 3d plot \n",
    "ax = Axes3D(fig)\n",
    "# set the data\n",
    "X1 = Target_Data\n",
    "\n",
    "# select X,Y, & Z columns\n",
    "xs = X1.iloc[:, 0]\n",
    "ys = X1.iloc[:, 1]\n",
    "zs = X1.iloc[:, 2]\n",
    "\n",
    "# set axes to 3D\n",
    "ax = plt.axes(projection = '3d')\n",
    "ax.scatter3D(xs, ys, zs,c=kmean.labels_) \n",
    "\n",
    "# Define label\n",
    "ax.set_xlabel('Recency')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_zlabel('Monetary')\n",
    "\n",
    "# Plot graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.4 Kmeans Cluster Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elbow method clearly states five distinguished clusters for the customers. The KMEAN plot indicates this segregation very clearly. The groups were formed by how recently the customers visited stores, their monetary impact, and their visit frequency.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "fig = px.scatter(scaled_df, y=\"Recency\", x=\"Frequency\", color=kmean.labels_, height=400, width=600)\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(scaled_df, x=\"Recency\", y=\"Monetary\", color=kmean.labels_, height=400, width=600)\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(scaled_df, x=\"Monetary\", y=\"Frequency\", color=kmean.labels_, height=400, width=600)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.5 RFM Scores per cluster  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(X_copy, x=\"Recency_score\", color=\"Cluster\", height=300, width=500)\n",
    "fig.show()\n",
    "fig = px.histogram(X_copy, x=\"Monetary_score\", color=\"Cluster\", height=300, width=500)\n",
    "fig.show()\n",
    "fig = px.histogram(X_copy, x=\"Frequency_score\", color=\"Cluster\", height=300, width=500)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Silhouette Score & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Generate silhouette score and plot the feature dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(Target_Data) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(Target_Data)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(Target_Data, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(Target_Data, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        Target_Data.iloc[:, 0], Target_Data.iloc[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the Recency\")\n",
    "    ax2.set_ylabel(\"Feature space for the Frequency\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # 3rd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax3.scatter(\n",
    "        Target_Data.iloc[:, 1], Target_Data.iloc[:, 2], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax3.scatter(\n",
    "        centers[:, 1],\n",
    "        centers[:, 2],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax3.scatter(c[1], c[2], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax3.set_title(\"The visualization of the clustered data.\")\n",
    "    ax3.set_xlabel(\"Feature space for the Frequency\")\n",
    "    ax3.set_ylabel(\"Feature space for the Monetary\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 Analysis of Silhouette coefficient and clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Silhouette coefficients seems to be decreasing as number of clusters are increasing, however the outliers started appearing from 5th cluster, hence 5 clusters seems to an optimal choice for cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 DBSCAN Alogorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.1 NearestNeighbor - Find optimal EPS parameter for DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select the optimal value of eps parameter for DBSCAN by executing Nearest neighbor algorithm. \n",
    "# \n",
    "# Import the nearest neighbor from Sklearn \n",
    "from sklearn.neighbors import NearestNeighbors # type: ignore\n",
    "\n",
    "# Initialize and fit the algorithm\n",
    "near_n = NearestNeighbors().fit(scaled_df)\n",
    "\n",
    "# Capture the distance and Index\n",
    "distances, indices = near_n.kneighbors(scaled_df)\n",
    "\n",
    "#Plot the distance and index. \n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(distances)\n",
    "plt.title(\"Nearest neighbor - distance/ indices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2 - Execute DBSCAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "# Import required libraries \n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "# Execute DBSCAN algorithm\n",
    "db = DBSCAN(eps=0.118, min_samples=10).fit(Target_Data)\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) \n",
    "n_noise_ = list(labels).count(-1) \n",
    "\n",
    "# Print estimated numbers of clusters\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.3 Plot DBSCAN result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = Target_Data[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy.iloc[:, 1],\n",
    "        xy.iloc[:, 2],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "#        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = Target_Data[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy.iloc[:, 1],\n",
    "        xy.iloc[:, 2],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "#        markersize=6,\n",
    "    )\n",
    "\n",
    "plt.title(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.4 Analyze DBSCAN result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With optimum EPS parameter based on nearest neighbor, the number of customer clusters from DBSCAN is 5, which is similar to other algorithms.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Customer Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 RFM score per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the plot to sum up score for customer clusters\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "fig.set_size_inches(20,6)\n",
    "\n",
    "# Plot first graph\n",
    "\n",
    "for col in ['Recency_score', 'Frequency_score', 'Monetary_score']:\n",
    "#   Create the dataframe of sum of score     \n",
    "    grp = pd.DataFrame(X.groupby('Cluster')[col].sum())\n",
    "    grp['Clusters']  = grp.index.values\n",
    "    grp.columns = ['Aggregation', 'Clusters']\n",
    "#   Set the axis labels\n",
    "    if col == 'Recency_score': \n",
    "        ax1.set_title(\"The \"f\"{col} score for customer clusters.\")\n",
    "        ax1.set_xlabel(\"Cluster Number\")\n",
    "        ax1.set_ylabel(\"Sum of \"f\"{col} score\")\n",
    "        ax1.bar(list(grp.iloc[:,1].values), list(grp.iloc[:,0].values))\n",
    "#   \n",
    "    if col == 'Frequency_score': \n",
    "        ax2.set_title(\"The \"f\"{col} score for customer clusters.\")\n",
    "        ax2.set_xlabel(\"Cluster Number\")\n",
    "        ax2.set_ylabel(\"Sum of \"f\"{col} score\")\n",
    "        ax2.bar(list(grp.iloc[:,1].values), list(grp.iloc[:,0].values))\n",
    "#   \n",
    "    if col == 'Monetary_score': \n",
    "        ax3.set_title(\"The \"f\"{col} score for customer clusters.\")\n",
    "        ax3.set_xlabel(\"Cluster Number\")\n",
    "        ax3.set_ylabel(\"Sum of \"f\"{col} score\")\n",
    "        ax3.bar(list(grp.iloc[:,1].values), list(grp.iloc[:,0].values))\n",
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Customer count per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Create the dataframe of sum of score     \n",
    "grp = pd.DataFrame(X_copy.groupby('Cluster')['CustomerID'].count())\n",
    "grp['Clusters']  = grp.index.values\n",
    "grp.columns = ['Aggregation', 'Clusters']\n",
    "\n",
    "fig = px.bar(grp, x='Clusters', y='Aggregation',height=400, width=600, text_auto=True, title='Count of customers per cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Peak purchase time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify peak selling time for Retailer grouped by cluster\n",
    "fig = px.histogram(X_copy, x=\"Time\", color=\"Cluster\", height=400, width=600, title='Purchase time by cluster').update_xaxes(categoryorder='total descending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Cluster wise Top selling Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "## Plot top selling month \n",
    "from pandas import to_datetime # type: ignore\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the date dataframe with feature as Cluster, Date & Quantity\n",
    "df_date =X_copy[['Cluster', 'Date', 'Quantity']]\n",
    "\n",
    "# Find the Month from date column \n",
    "df_date['Month'] = to_datetime(df_date['Date']).dt.strftime('%B')\n",
    "\n",
    "# Create subplot\n",
    "fig = make_subplots(rows=3, cols=2,\n",
    "                    subplot_titles=(\"Customer Cluster 0\",\"Customer Cluster 1\", \"Customer Cluster 2\",\n",
    "                                    \"Customer Cluster 3\",\"Customer Cluster 4\", \"All Clusters\"))\n",
    "\n",
    "# Initialize the cluster variable \n",
    "clus = 0\n",
    "\n",
    "# Loop to generate the line plot in 3x2 frame. \n",
    "for i in list(range(1,4,1)):\n",
    "    for j in list(range(1,3,1)):\n",
    "        if i == 3 and j==2:\n",
    "            df_date1 =df_date.groupby(['Month'])[['Quantity']].sum().sort_values(by='Quantity')\n",
    "            xd = df_date1.index.values\n",
    "            yd = df_date1['Quantity']\n",
    "            fig.add_trace(go.Line(x=xd, y=yd),row=i, col=j)\n",
    "            fig.update_layout(height=800, width=1000, title_text=\"Cluster Wise Top Selling Month\")\n",
    "            fig.show()\n",
    "        else: \n",
    "            df_date1 =df_date.query(\"Cluster == \"f\"{clus}\").groupby(['Month'])[['Quantity']].sum().sort_values(by='Quantity')\n",
    "            xd = df_date1.index.values\n",
    "            yd = df_date1['Quantity']\n",
    "            fig.add_trace(go.Line(x=xd, y=yd),row=i, col=j)\n",
    "            clus = clus+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Top Selling items per Cluster and Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += '<div style=\"display:inline-block; margin-right: 25px;\">' \n",
    "        html_str += df.to_html()\n",
    "        html_str += '</div>'\n",
    "    display(HTML(html_str))\n",
    "\n",
    "for cls in list(range(0,6,1)):\n",
    "    df_name= f\"gls_{cls}\"\n",
    "    if cls == 5:\n",
    "        globals()[df_name] = X_copy[['Description','Quantity']].groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "        globals()[df_name] = globals()[df_name].to_frame()\n",
    "        globals()[df_name] = globals()[df_name].style.set_caption(\"OverAll\")\n",
    "        print('Top Selling Item in each Segment & over all')\n",
    "        display_side_by_side(gls_0,gls_1) # type: ignore\n",
    "        display_side_by_side(gls_2,gls_3) # type: ignore\n",
    "        display_side_by_side(gls_4,gls_5) # type: ignore \n",
    "    else:\n",
    "        globals()[df_name] = X_copy.query(\"Cluster == \"f\"{cls}\")[['Description','Quantity']].groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "        globals()[df_name] = globals()[df_name].to_frame()\n",
    "        globals()[df_name] = globals()[df_name].style.set_caption(\"Segment\"f\"{cls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot to represent the Top sold StockID sold across cluster, and overall  \n",
    "\n",
    "# Set the plot frame\n",
    "fig, [(ax1, ax2), (ax3, ax4), (ax5, ax6)] = plt.subplots(3, 2)\n",
    "fig.set_size_inches(40,20)\n",
    "\n",
    "# Function to get the aggregrated value based on customer cluster number\n",
    "def conv_df(cls):\n",
    "    gls = X_copy.query(\"Cluster == \"f\"{cls}\")[['StockCode','Quantity']].groupby('StockCode')['Quantity'].sum().sort_values(ascending=False).head(20)\n",
    "    gls = gls.to_frame()\n",
    "    gls['sc'] = gls.index.values\n",
    "    gls.columns = ['sc', 'scode']\n",
    "    return gls\n",
    "\n",
    "# Loop to plot the top seeling stockID\n",
    "for cluster in list(range(0,6,1)):\n",
    "    if cluster == 5:\n",
    "        dfm = conv_df(list(range(0,6,1)))\n",
    "        ax6.set_title(\"Top sold Items across all customer segments\")\n",
    "        ax6.set_xlabel(\"Product Number\")\n",
    "        ax6.set_ylabel(\"Product sold count\")\n",
    "        ax6.bar(dfm['scode'], dfm['sc'], color='m')\n",
    "\n",
    "    if cluster == 0:\n",
    "        dfm = conv_df(0)\n",
    "        ax1.set_title(\"Top sold Items across Customer Segment-0\")\n",
    "        ax1.set_xlabel(\"ClPuster Number\")\n",
    "        ax1.set_ylabel(\"Product sold count\")\n",
    "        ax1.bar(dfm['scode'], dfm['sc'], color='b')\n",
    "    \n",
    "    if cluster == 1:\n",
    "        dfm = conv_df(1)\n",
    "        ax2.set_title(\"Top sold Items across Customer Segment-1\")\n",
    "        ax2.set_xlabel(\"ClPuster Number\")\n",
    "        ax2.set_ylabel(\"Product sold count\")\n",
    "        ax2.bar(dfm['scode'], dfm['sc'], color='g')\n",
    "    \n",
    "    if cluster == 2:\n",
    "        dfm = conv_df(2)\n",
    "        ax3.set_title(\"Top sold Items across Customer Segment-2\")\n",
    "        ax3.set_xlabel(\"ClPuster Number\")\n",
    "        ax3.set_ylabel(\"Product sold count\")\n",
    "        ax3.bar(dfm['scode'], dfm['sc'], color='c')\n",
    "    \n",
    "    if cluster == 3:\n",
    "        dfm = conv_df(3)\n",
    "        ax4.set_title(\"Top sold Items across Customer Segment-3\")\n",
    "        ax4.set_xlabel(\"ClPuster Number\")\n",
    "        ax4.set_ylabel(\"Product sold count\")\n",
    "        ax4.bar(dfm['scode'], dfm['sc'], color='y')\n",
    "    \n",
    "    if cluster == 4:\n",
    "        dfm = conv_df(4)\n",
    "        ax5.set_title(\"Top sold Items across Customer Segment-4\")\n",
    "        ax5.set_xlabel(\"ClPuster Number\")\n",
    "        ax5.set_ylabel(\"Product sold count\")\n",
    "        ax5.bar(dfm['scode'], dfm['sc'], color= 'k')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Tree Map - Customer Segement Based On RFM Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.6.1 RFM Score based Segment Tree-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate score based Segement TreeMap\n",
    "\n",
    "X_copy['RFM_Segment'] = X_copy['Recency_score'].astype(str) + X_copy['Frequency_score'].astype(str) + X_copy['Monetary_score'].astype(str)\n",
    "segment_counts = X_copy['RFM_Segment'].value_counts().reset_index()\n",
    "segment_counts.columns = ['RFM_Segment', 'Count']\n",
    "\n",
    "# Plot the segment MAP\n",
    "fig = px.treemap(segment_counts, path=['RFM_Segment'], values='Count', title='RFM Segmentation for Customer(Customer ID)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.6.2 Create Customer Type Column Based On RFM Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the customer based RMF score. The overall RFM score is concatenation of individual string value from R+F+M score. \n",
    "# The reference of this score is taken from blog/internet, however defining customer based on RFM score is business decision and # can vary based on organizations.\n",
    "\n",
    "# Define customers on RFM score\n",
    "#  \n",
    "CHAMPIONS = ['555','554','544','545','454','455''445']\n",
    "LOYAL = ['543','444','435','355','354','345','344''335']\n",
    "POTENTIAL_LOYALIST = ['553','551','552','541','542','533','532','531','452','451','442','441','431','453','433','432','423','353','352','351','342','341','333','323']\n",
    "RECENT_CUSTOMERS = ['512','511','422','421','412','411','311']\n",
    "PROMISING = ['525','524','523','522','521','515','514','513','425','424','413','414','415','315','314','313']\n",
    "NEED_ATTENTION = ['535','534','443','434','343','334','325','324']\n",
    "ABOUT_TO_SLEEP = ['331','321','312','221','213','231','241','251']\n",
    "AT_RISK = ['255','254','245','244','253','252','243','242','235','234','225','224','153','152','145','143','142','135','134','133','125','124']\n",
    "CANNOT_LOSE = ['155','154','144','214','215','115','114','113']\n",
    "HIBERNATING = ['332','322','231','241','251','233','232','223','222','132','123','122','212','211']\n",
    "LOST = ['111','112','121','131','141','151']\n",
    "\n",
    "\n",
    "# Add new column CustType based on individual score of customer \n",
    "X_copy['CustType'] = X_copy['RFM_Segment'].apply(\n",
    "    lambda i: 'CHAMPIONS' if i in CHAMPIONS else\n",
    "              'LOYAL' if i in LOYAL else\n",
    "              'POTENTIAL_LOYALIST' if i in POTENTIAL_LOYALIST else\n",
    "              'RECENT_CUSTOMERS' if i in RECENT_CUSTOMERS else\n",
    "              'PROMISING' if i in PROMISING else\n",
    "              'NEED_ATTENTION' if i in NEED_ATTENTION else\n",
    "              'ABOUT_TO_SLEEP' if i in ABOUT_TO_SLEEP else\n",
    "              'AT_RISK' if i in AT_RISK else\n",
    "              'CANNOT_LOSE' if i in CANNOT_LOSE else\n",
    "              'HIBERNATING' if i in HIBERNATING else\n",
    "              'LOST' if i in LOST else\n",
    "              'ERR'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.6.3 Tree MAP for Customer Segement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Draw tree Map\n",
    "\n",
    "# Calculate the customer segment count\n",
    "cust_segment_counts = X_copy['CustType'].value_counts().reset_index()\n",
    "cust_segment_counts.columns = ['RFM_Cust_Segment', 'Count']\n",
    "\n",
    "# Draw the segment MAP\n",
    "fig = px.treemap(cust_segment_counts, path=['RFM_Cust_Segment'], values='Count', title='RFM Segmentation for Customer')\n",
    "fig.show()\n",
    "\n",
    "## Note we can also draw the segement on individual cluster to focus customers in individual cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying multiple models for Customer clustering, almost all the models converge to similar results with the same number of optimal customer clusters. Using the RFM feature, customers are further clustered in various focus areas and served based on individual choices. Identified the peak selling day/ time for the customers, along with the top-selling items for the customer segment.\n",
    "\n",
    "Cluster 1 has the highest count of the number of customers and customers in this segment are the most frequent visitors to the website, and contribute more in monetary value. More business analysis is required on the cluster 2 & 3 customers as they were frequent visitors and have contributed fairly in terms of monetary value to the website, however, due to some reason they have not visited the website as their recency score is way lower than other segment.\n",
    "\n",
    "Lastly have categorized the customers with RFM scores in multiple segments like potential loyalists, need attention, lost customers, and so on. The business can target individual segments to generate more sales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
